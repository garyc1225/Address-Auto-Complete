{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address-Autocomplete-Bot\n",
    "\n",
    "This notebook shows the end-to-end process of training and building a Address Autocomplete Bot using Global Attention Mechanism. This work was inspired by the __[Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)](https://arxiv.org/abs/1508.04025v5)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Idea](#1)\n",
    "- [2 - Review](#2)\n",
    "- [3 - Address Datasets](#3)\n",
    "\n",
    "- [2 - Neural Machine Translation with Attention](#2)\n",
    "    - [2.1 - Attention Mechanism](#2-1)\n",
    "        - [Exercise 1 - one_step_attention](#ex-1)\n",
    "        - [Exercise 2 - modelf](#ex-2)\n",
    "        - [Exercise 3 - Compile the Model](#ex-3)\n",
    "- [3 - Visualizing Attention (Optional / Ungraded)](#3)\n",
    "    - [3.1 - Getting the Attention Weights From the Network](#3-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<a id=\"1\"></a>\n",
=======
    "<a name='1'></a>\n",
>>>>>>> f2e6f3af7201ea0cf2f4ed1e8cbbed6c1c606a2b
    "## 1 - Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address is one of the most important pieces of data to get right for all transactions in life. When we input the correct address into a website, we ensure that the products are delivered to the right address, improve customer satisfaction, and reduce the risk of fraud. My wife and I buy hundreds of items across dozens of websites, and it's hard to imagine what would happen if the address was incorrect.\n",
    "\n",
    "Luckily, Google has services like the [Maps API](https://developers.google.com/maps/documentation/javascript), which eliminates these concerns. These map APIs are widely available, lightweight, and easy to deploy. They can be used to build dynamic and interactive maps for web applications using geospatial data, ensuring that customers never enter the wrong address again. For example, the [Place Autocomplete Address Form](https://developers.google.com/maps/documentation/javascript/examples/places-autocomplete-addressform) helps to accurately supply address details:\n",
    "\n",
    "\n",
    "> The Place Autocomplete Address Form sample captures selected address components from the Google Places database, and uses them to populate an address form.\n",
    "\n",
    "This project aims to build an address autocomplete bot that uses the attention mechanism to autocomplete addresses without any knowledge of geospatial information. The bot should be able to detect inaccurate addresses and autocomplete them by filling in the missing address components or correcting the wrong address components. The attention mechanism allows the neural network to focus on the relative important parts of an address, such as the street name, city, and state, without knowing the difference between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<a id=\"2\"></a>\n",
=======
    "<a name='2'></a>\n",
>>>>>>> f2e6f3af7201ea0cf2f4ed1e8cbbed6c1c606a2b
    "## 2 - Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods are being used in solving NLP quesitons since the early 2000s. Deep learning methods are a type of neural network that can learn complex relationships in data. In 2010s, we witness the rise of large language models (LLMs). LLMs are trained on massive datasets of text and code, which allows them to learn the statistical relationships between words and phrases at an unprecedented scale. In the paragraph below, I will summarize the development of deep learning methods on NLP, from Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), to the recent developed Attention Mechanism. \n",
    "\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "RNN are a type of neural network that can process sequences of data. They are the first type of neural networks that are able to learn long-term dependencies within text data, making them the first choice to solve NLP problems. However, one of the disadvantages of RNN is that they are prone to \"vanishing gradient\". The vanishing gradient problem is a major obstacle to training RNNs to learn long-term dependencies. This is because long-term dependencies require the network to remember information from many time steps ago, and the vanishing gradient problem makes it difficult for the network to do this.\n",
    "\n",
    "A language example can easily demonstrate this idea of long-term dependencies. In the sentence below, the verb *\"has\"* is influenced by the word *\"Dog\"* at the very beginning. If the word *\"Dog\"* is changed to a plural form, then the verb *\"has\"* would need to be updated to the word *\"have\"*. This long-range dependencies can extend over a very long step, as we do not know how many words (aka steps) are in between these two words.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/language_example.png\" width=\"500\"> <br>\n",
    "<caption><center><b>Figure 1</b>: A language example </center></caption>\n",
    "</div>\n",
    "\n",
    "A standard RNN neural network architecture is shown below. In order to learn the long-term dependencies relationship, we hope the model is able to pass on the hidden state $a^{\\langle 2 \\rangle}$ to the position of $x^{\\langle T_{x} \\rangle}$. The basic RNNs that we have seen so far are not very good at handling such long-term dependencies, mainly due to the Vanishing Gradient Problem. When the gradients traverse through multiple steps it become very small, which makes it difficult for the network to learn.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/rnn.png\" width=\"500\"> <br>\n",
    "<caption><center><b>Figure 2</b>: Basic RNN Structure </center></caption>\n",
    "</div>\n",
    "\n",
    "- **Long Short-Term Memory (LSTM)**\n",
    "\n",
    "LSTMs are a type of RNN that address the vanishing gradient problem. They do this by using gates to control the flow of information in the network. The figure below shows the operations of an LSTM cell.\n",
    "\n",
    "The gates in a LSTM cell are described below:\n",
    "\n",
    "- Forget Gate ($\\mathbf{\\Gamma}_{f}$): The forget gate can be used to *\"forget\"* the previous state. For example, if the subject changes from a singular word *\"Dog\"* to a plural *\"Dogs\"*, the memory of the previous state becomes outdated and should be forgotten. \n",
    "\n",
    "- Update Gate ($\\mathbf{\\Gamma}_{i}$): The update gate can be used to decide what aspects of the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to the cell state $c^{\\langle t \\rangle}$. The candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is a tensor containing information from the current time step that **may** be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$. The current cell state $\\mathbf{c}^{\\langle t \\rangle}$ is the \"memory\" that gets passed onto future time steps.\n",
    "\n",
    "- Output Gate ($\\mathbf{\\Gamma}_{o}$): The output gate decides what gets sent as the prediction (output) of the time step.\n",
    "\n",
    "The LSTM unit can remember long-term dependencies because the cell state is not updated at every time step, as shown in formula 4. This allows the LSTM unit to retain information from previous time steps, even if it is not used in the current time step.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/lstm.png\" width=\"500\"> <br>\n",
    "<caption><center><b>Figure 3</b>: LSTM Cell </center></caption>\n",
    "</div>\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "- **Attention Mechanism**\n",
    "\n",
    "LSTM solves the vanishing gradient problem as it allows the cell state to flow through multiple time steps. However, in the language example above, the word *\"has\"* does not rely on any other word in the sentence other than the word *\"dog\"*. Attention Mechanism was developed to allow the neural network to put more weight on certain long-range dependencies than others at each position $x^{\\langle T_{x} \\rangle}$, hence the word \"Attention\".\n",
    "\n",
    "The attention mechanism was first proposed by __[Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2014)](https://arxiv.org/abs/1409.0473)__, and its a powerful tool that has revolutionized the field of NLP.  Attention Mechanism is commonly used with the encoder-decoders network family. The encoder encodes a source sentence into a fixed-length tensor. The attention mechanism then calcualtes a score for each word, indicating how important the word is to the current position $x^{\\langle T_{x} \\rangle}$. When the score is higher, the network will pay more attention to this word for the current output $y^{\\langle T_{x} \\rangle}$. When the score is lower, the network pay less attention.\n",
    "\n",
    "The figure below shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$. $s^{\\langle t-1 \\rangle}$ is the one-step prior hidden state from the post-attention decoder, and $a^{\\langle t' \\rangle}$ is the hidden state from the pre-attention encoder. $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network with a dense layer to learn and compute the output $e^{\\langle t, t' \\rangle}$. $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $\\alpha^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$. \n",
    "\n",
    "Finally, the $context^{ \\langle t \\rangle }$ works as a weighted average of all the attention weights. Then, the decoder's output along with the context vector is used to predict the next output $y^{\\langle T_{x+1} \\rangle}$\n",
    "\n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"image/attn_mechanism.png\" width=\"500\"> <br>\n",
    "<caption><center><b>Figure 4</b>: Attention Mechanism </center></caption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
